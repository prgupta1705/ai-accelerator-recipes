{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6285d85a00d6a31792fac4c08ba523cd049cf07"
   },
   "source": [
    "# Bank Loan Status\n",
    "This use case helps in determining the Loan Status of a customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "This dataset is taken from public domain. It consists of 19 columns. They are:\n",
    "\n",
    "1.Loan ID\n",
    "\n",
    "2.Customer ID\n",
    "\n",
    "3.Loan Status\n",
    "\n",
    "4.Current Loan Amount\n",
    "\n",
    "5.Term\n",
    "\n",
    "6.Credit Score\n",
    "\n",
    "7.Annual Income\n",
    "\n",
    "8.Years in current job\n",
    "\n",
    "9.Home Ownership\n",
    "\n",
    "10.Purpose\n",
    "\n",
    "11.Monthly Debt\n",
    "\n",
    "12.Years of Credit History\n",
    "\n",
    "13.Months since last delinquent\n",
    "\n",
    "14.Number of Open Accounts\n",
    "\n",
    "15.Number of Credit Problems\n",
    "\n",
    "16.Current Credit Balance\n",
    "\n",
    "17.Maximum Open Credit\n",
    "\n",
    "18.Bankruptcies\n",
    "\n",
    "19.Tax Liens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "##### # Imports\n",
    "\n",
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# No warnings about setting value on copy of slice\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Display up to 60 columns of a dataframe\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "# Matplotlib visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default font size\n",
    "plt.rcParams['font.size'] = 24\n",
    "\n",
    "# Internal ipython tool for setting figure size\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Seaborn for visualization\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# Splitting data into training and testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# # # Data Cleaning and Formatting\n",
    "\n",
    "# # Load in the Data and Examine\n",
    "\n",
    "# Read in credit into a dataframe \n",
    "credit = pd.read_csv('../input/credit_train.csv')\n",
    "\n",
    "# Display top of dataframe\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dd5c8d62615e1e852c59db262dde7fef027ca06"
   },
   "outputs": [],
   "source": [
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2a8dd84e67164754d6d43902592a8249ec6dac8"
   },
   "outputs": [],
   "source": [
    "# # Data Types and Missing Values\n",
    "\n",
    "# See the column data types and non-missing values\n",
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c722bd0380292d587e258da5e3ff09d00f57bca"
   },
   "outputs": [],
   "source": [
    "# Statistics for each column\n",
    "credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e516824b86742c41bce64e70d6134231e31a4ea3"
   },
   "outputs": [],
   "source": [
    "credit.drop(labels=['Loan ID', 'Customer ID'], axis=1, inplace=True)\n",
    "\n",
    "# These two features are only for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0d448aee129b1a9d2dfbfe60e7ffa71c3011689"
   },
   "outputs": [],
   "source": [
    "# # Missing Values\n",
    "\n",
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "254bd2f8d7e4e8082f412dfe7660f63dad43dc14"
   },
   "outputs": [],
   "source": [
    "missing_values_table(credit)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52ccf316384829ebc2b01d27ccf4b40aef37403c"
   },
   "outputs": [],
   "source": [
    "# Drop the columns with > 50% missing\n",
    "credit.drop(columns = 'Months since last delinquent', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "330d0ef3e7e0732b90dccaeb56d8ce949b48b509",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit[credit['Years of Credit History'].isnull() == True]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72df239d275ad726663669430043430d43477bec"
   },
   "outputs": [],
   "source": [
    "credit.drop(credit.tail(514).index, inplace=True) # drop last 514 rows\n",
    "missing_values_table(credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f37c96f849e1a0505bf6dab39e377d115957662a"
   },
   "outputs": [],
   "source": [
    "# As the number of missing values is so low in the 'Maximum Open Credit' we will drop them.\n",
    "\n",
    "for i in credit['Maximum Open Credit'][credit['Maximum Open Credit'].isnull() == True].index:\n",
    "    credit.drop(labels=i, inplace=True)\n",
    "missing_values_table(credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1400de3e7f5be2188f94b6b41723ce8976adc3d2"
   },
   "outputs": [],
   "source": [
    "# As the number of missing values is so low in the 'Tax Liens' we will drop them.\n",
    "\n",
    "for i in credit['Tax Liens'][credit['Tax Liens'].isnull() == True].index:\n",
    "    credit.drop(labels=i, inplace=True)\n",
    "missing_values_table(credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61a666542aade522dbb48c908dcf7caf7ecd603d"
   },
   "outputs": [],
   "source": [
    "# As the number of missing values is so low in the 'Bankruptcies' we will drop them.\n",
    "\n",
    "for i in credit['Bankruptcies'][credit['Bankruptcies'].isnull() == True].index:\n",
    "    credit.drop(labels=i, inplace=True)\n",
    "missing_values_table(credit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a33a945216de8d42bdbf3731fdfe2d439693ee9"
   },
   "outputs": [],
   "source": [
    "# Use the 'mean' technique to fill the NaN values.\n",
    "\n",
    "credit.fillna(credit.mean(), inplace=True)\n",
    "missing_values_table(credit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "430680323b51481400716a6598c95ec555ec8b55"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "sns.countplot(credit['Years in current job'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89356ed83e426d2412294230cbcf0d3d5297aa01"
   },
   "outputs": [],
   "source": [
    "credit.fillna('10+ years', inplace=True) # fill with '10+ years'.\n",
    "missing_values_table(credit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfbfe9c6e65a4de3890e5a3c4d56142c9dc73703",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # # Exploratory Data Analysis\n",
    "\n",
    "sns.pairplot(credit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fac1793f054519d8f11c418cd1b383b73d4777a"
   },
   "outputs": [],
   "source": [
    "# # Correlations between Features and Target\n",
    "\n",
    "# Find all correlations and sort \n",
    "correlations_data = credit.corr()['Credit Score'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations\n",
    "print(correlations_data.tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ef51251b405bfecdc0e6c481f6a54eecd009c94"
   },
   "outputs": [],
   "source": [
    "# # # Feature Engineering and Selection\n",
    "\n",
    "credit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13e110a0d27ac1da28d562ad0b6d84d8bfd4cd75"
   },
   "outputs": [],
   "source": [
    "# # Encoding categorical data & Feature Scaling\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_subset = credit[['Term', 'Years in current job', 'Home Ownership', 'Purpose']]\n",
    "\n",
    "# One hot encode\n",
    "categorical_subset = pd.get_dummies(categorical_subset)\n",
    "\n",
    "# Join the dataframe in credit_train\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "# First we will drop the 'old' categorical datas and later join the 'new' one.\n",
    "\n",
    "credit.drop(labels=['Term', 'Years in current job', 'Home Ownership', 'Purpose'], axis=1, inplace=True)\n",
    "credit = pd.concat([credit, categorical_subset], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b07ba1398ad974d62ccd1c65a1981915abc6976f"
   },
   "outputs": [],
   "source": [
    "# #  Remove Collinear Features\n",
    "\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    y = x['Loan Status']\n",
    "    x = x.drop(columns = ['Loan Status'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    \n",
    "    # Add the score back in to the data\n",
    "    x['Loan Status'] = y\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ed921620417e13fb5641564b5090b3d7e97d86d"
   },
   "outputs": [],
   "source": [
    "# Remove the collinear features above a specified correlation coefficient\n",
    "credit = remove_collinear_features(credit, 0.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4be0e65c2ede81decd769b8e8768194a3c56b4cf"
   },
   "outputs": [],
   "source": [
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "241e10723db0c1fd8df28e7afbac69d8b2b4ab1b"
   },
   "outputs": [],
   "source": [
    "# # # Split Into Training and Testing Sets\n",
    "\n",
    "# Separate out the features and targets\n",
    "features = credit.drop(columns='Loan Status')\n",
    "targets = pd.DataFrame(credit['Loan Status'])\n",
    "\n",
    "# Split into 80% training and 20% testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "349e486f1e8525e364f6be5334291ae8f00ca0cd"
   },
   "outputs": [],
   "source": [
    "# # Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_y_train = LabelEncoder()\n",
    "y_train = labelencoder_y_train.fit_transform(y_train)\n",
    "labelencoder_y_test = LabelEncoder()\n",
    "y_test = labelencoder_y_test.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5fb7a46a004a2ec0570c4c135088a447c825d67"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3eadeec18118a0f19acb201dc89d2d617617c8e"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fc8657bd8f88e32c876056e8279a1982a7d51bd"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "102f279c83b538955a51028ea6b589cd9c75d293"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03122062d1f1f41806b73d724ecc2cf403894b06"
   },
   "outputs": [],
   "source": [
    "# # # Models to Evaluate\n",
    "\n",
    "# We will compare five different machine learning Classification models:\n",
    "\n",
    "# 1 - Logistic Regression\n",
    "# 2 - K-Nearest Neighbors Classification\n",
    "# 3 - Suport Vector Machine\n",
    "# 4 - Naive Bayes\n",
    "# 5 - Random Forest Classification\n",
    "\n",
    "# Function to calculate mean absolute error\n",
    "def cross_val(X_train, y_train, model):\n",
    "    # Applying k-Fold Cross Validation\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n",
    "    return accuracies.mean()\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_cross = cross_val(X_train, y_train, model)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return model_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8fa7e51ce560cc110a94fd0d3988f7eefa6f26d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "logr_cross = fit_and_evaluate(logr)\n",
    "\n",
    "print('Logistic Regression Performance on the test set: Cross Validation Score = %0.4f' % logr_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60e1016bc91b18ebabd1b0cac4585260d5ccd924"
   },
   "outputs": [],
   "source": [
    "# # K-NN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "knn_cross = fit_and_evaluate(knn)\n",
    "\n",
    "print('KNN Performance on the test set: Cross Validation Score = %0.4f' % knn_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "760b7b5bb05d9f130246180ca97c21ddcd67600c"
   },
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive = GaussianNB()\n",
    "naive_cross = fit_and_evaluate(naive)\n",
    "\n",
    "print('Naive Bayes Performance on the test set: Cross Validation Score = %0.4f' % naive_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae4b09b6570c6aa3f9928bc21ae93dbe7066362c"
   },
   "outputs": [],
   "source": [
    "# # Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "random_cross = fit_and_evaluate(random)\n",
    "\n",
    "print('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d98eaec33f4e6cadd299ec586ab840020952b6fb"
   },
   "outputs": [],
   "source": [
    "# # Gradiente Boosting Classification\n",
    "from xgboost import XGBClassifier\n",
    "gb = XGBClassifier()\n",
    "gb_cross = fit_and_evaluate(gb)\n",
    "\n",
    "print('Gradiente Boosting Classification Performance on the test set: Cross Validation Score = %0.4f' % gb_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4413692210395764ecf6bdaa68678983b7106bc"
   },
   "outputs": [],
   "source": [
    "# Now, to better understand the results, we will see in a graph the model that has the better Cross Validation Score\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "figsize=(8, 6)\n",
    "\n",
    "# Dataframe to hold the results\n",
    "model_comparison = pd.DataFrame({'model': ['Logistic Regression', 'K-NN',\n",
    "                                           'Naive Bayes', 'Random Forest',\n",
    "                                            'Gradiente Boosting'],\n",
    "                                 'score': [logr_cross, knn_cross, naive_cross, \n",
    "                                         random_cross, gb_cross]})\n",
    "\n",
    "# Horizontal bar chart of test mae\n",
    "model_comparison.sort_values('score', ascending = True).plot(x = 'model', y = 'score', kind = 'barh',\n",
    "                                                           color = 'red', edgecolor = 'black')\n",
    "\n",
    "# Plot formatting\n",
    "plt.ylabel(''); plt.yticks(size = 14); plt.xlabel('K-Fold Cross Validation'); plt.xticks(size = 14)\n",
    "plt.title('Model Comparison on Score', size = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68c0e5f787870535c909005d66113a2651d12830"
   },
   "outputs": [],
   "source": [
    "# # # Model Optimization\n",
    "\n",
    "# # Hyperparameter\n",
    "\n",
    "# Hyperparameter Tuning with Random Search and Cross Validation\n",
    "\n",
    "# Here we will implement random search with cross validation to select the optimal hyperparameters for the gradient boosting regressor. \n",
    "# We first define a grid then peform an iterative process of: randomly sample a set of hyperparameters from the grid, evaluate the hyperparameters using 4-fold cross-validation, \n",
    "# and then select the hyperparameters with the best performance.\n",
    "\n",
    "# Loss function to be optimized\n",
    "loss = ['ls', 'lad', 'huber']\n",
    "\n",
    "# Number of trees used in the boosting process\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "\n",
    "# Maximum depth of each tree\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "\n",
    "# Minimum number of samples per leaf\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "# Minimum number of samples to split a node\n",
    "min_samples_split = [2, 4, 6, 10]\n",
    "\n",
    "# Maximum number of features to consider for making splits\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {'loss': loss,\n",
    "                       'n_estimators': n_estimators,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea9ec545e6debd1c3b5c726ab5a63b5230b1955e"
   },
   "outputs": [],
   "source": [
    "# In the code below, we create the Randomized Search Object passing in the following parameters:\n",
    "\n",
    "#    estimator: the model\n",
    "#    param_distributions: the distribution of parameters we defined\n",
    "#    cv the number of folds to use for k-fold cross validation\n",
    "#    n_iter: the number of different combinations to try\n",
    "#    scoring: which metric to use when evaluating candidates\n",
    "#    n_jobs: number of cores to run in parallel (-1 will use all available)\n",
    "#    verbose: how much information to display (1 displays a limited amount)\n",
    "#    return_train_score: return the training score for each cross-validation fold\n",
    "#    random_state: fixes the random number generator used so we get the same results every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb8d4b598cb6115f8afa29fed29a1353bcb4ad94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The Randomized Search Object is trained the same way as any other scikit-learn model. \n",
    "# After training, we can compare all the different hyperparameter combinations and find the best performing one.\n",
    "\n",
    "# Create the model to use for hyperparameter tuning\n",
    "model = XGBClassifier(random_state = 42)\n",
    "\n",
    "# Set up the random search with 4-fold cross validation\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "random_cv = RandomizedSearchCV(estimator=model,\n",
    "                               param_distributions=hyperparameter_grid,\n",
    "                               cv=4, n_iter=25, \n",
    "                               scoring = 'neg_mean_absolute_error',\n",
    "                               n_jobs = -1, verbose = 1, \n",
    "                               return_train_score = True,\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72cd906f1b667e12946a44102979d8ef55981c7f"
   },
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "random_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd8d837b0f6bd084cf29090a89f1fc4368f1df64"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn uses the negative mean absolute error for evaluation because it wants a metric to maximize. \n",
    "# Therefore, a better score will be closer to 0. We can get the results of the randomized search into a dataframe, and sort the values by performance.\n",
    "\n",
    "# Get all of the cv results and sort by the test performance\n",
    "random_results = pd.DataFrame(random_cv.cv_results_).sort_values('mean_test_score', ascending = False)\n",
    "\n",
    "random_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "922388d85784fec033d756c9c0843c0c91a759e7"
   },
   "outputs": [],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d83a5cec0d161170d31dfa2b46a0d674a787c4d0"
   },
   "outputs": [],
   "source": [
    "# The best gradient boosted model has the following hyperparameters:\n",
    "\n",
    "# loss = ls\n",
    "# n_estimators = 100\n",
    "# max_depth = 5\n",
    "# min_samples_leaf = 6\n",
    "# min_samples_split = 2\n",
    "# max_features = auto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3db261e84daf6c4b9777588879ca0a92c8a5e632"
   },
   "outputs": [],
   "source": [
    "# We will focus on a single one, the number of trees in the forest (n_estimators).\n",
    "# By varying only one hyperparameter, we can directly observe how it affects performance. \n",
    "# In the case of the number of trees, we would expect to see a significant affect on the amount of under vs overfitting.\n",
    "\n",
    "# Here we will use grid search with a grid that only has the n_estimators hyperparameter. \n",
    "# We will evaluate a range of trees then plot the training and testing performance to get an idea of what increasing the number of trees does for our model. \n",
    "# We will fix the other hyperparameters at the best values returned from random search to isolate the number of trees effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c59406726362d253d910af2f3103608a4a1ab847"
   },
   "outputs": [],
   "source": [
    "# Create a range of trees to evaluate\n",
    "trees_grid = {'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]}\n",
    "\n",
    "model = XGBClassifier(loss = 'ls', max_depth = 5,\n",
    "                                  min_samples_leaf = 6,\n",
    "                                  min_samples_split = 2,\n",
    "                                  max_features = 'auto',\n",
    "                                  random_state = 42)\n",
    "\n",
    "# Grid Search Object using the trees range and the random forest model\n",
    "grid_search = GridSearchCV(estimator = model, param_grid=trees_grid, cv = 4, \n",
    "                           scoring = 'neg_mean_absolute_error', verbose = 1,\n",
    "                           n_jobs = -1, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ea5130e0e4e841bd3265f109eaaf0b457edc4ba"
   },
   "outputs": [],
   "source": [
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab0245973e91237f0eec4f844438b4acc9d62e56"
   },
   "outputs": [],
   "source": [
    "# Get the results into a dataframe\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Plot the training and testing error vs number of trees\n",
    "figsize=(8, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.plot(results['param_n_estimators'], -1 * results['mean_test_score'], label = 'Testing Error')\n",
    "plt.plot(results['param_n_estimators'], -1 * results['mean_train_score'], label = 'Training Error')\n",
    "plt.xlabel('Number of Trees'); plt.ylabel('Mean Abosolute Error'); plt.legend();\n",
    "plt.title('Performance vs Number of Trees');\n",
    "\n",
    "# There will always be a difference between the training error and testing error (the training error is always lower) but if there is a significant difference, \n",
    "# we want to try and reduce overfitting, either by getting more training data or reducing the complexity of the model through hyperparameter tuning or regularization.\n",
    "\n",
    "# For now, we will use the model with the best performance and accept that it may be overfitting to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7e28c06eb71850cc45c974d92ae0cd28ce040b2"
   },
   "outputs": [],
   "source": [
    "results.sort_values('mean_test_score', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70d076ed10f7ae5b7f280d4e06c6b6d46999d5b3"
   },
   "outputs": [],
   "source": [
    "# # # Evaluate Final Model on the Test Set\n",
    "\n",
    "# We will use the best model from hyperparameter tuning to make predictions on the testing set.\n",
    "\n",
    "# For comparison, we can also look at the performance of the default model. The code below creates the final model, trains it (with timing), and evaluates on the test set.\n",
    "\n",
    "# Default model\n",
    "default_model = XGBClassifier(random_state = 42)\n",
    "\n",
    "# Select the best model\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f57808f6cc35053386fe04a29181bb31bd5d7468"
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 5\n",
    "default_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0679eeb425ab9195402cbc9ec0b1604981aa65de"
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 5\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e22420e5d2d8928d6364322ab1f06c5db5fd9d77"
   },
   "outputs": [],
   "source": [
    "default_pred = default_model.predict(X_test)\n",
    "final_pred = final_model.predict(X_test)\n",
    "\n",
    "# Function to calculate mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "print('Default model performance on the test set: MAE = %0.4f.' % mae(y_test, default_pred))\n",
    "print('Final model performance on the test set:   MAE = %0.4f.' % mae(y_test, final_pred))\n",
    "\n",
    "# The final model does out-perform the baseline model by about less than 1% and at the cost of significantly increased running time (it's about 7 times slower on my machine). \n",
    "\n",
    "# Here, the increase in run time is not an impediment, because while the relative difference is very small, the absolute magnitude of the training time is not significant. \n",
    "# In a different situation, the balance might not be the same so we would need to consider what we are optimizing for and the limitations we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b59237f724d86418c50cf353a94fcd44f4abd08"
   },
   "outputs": [],
   "source": [
    "# To get a sense of the predictions, we can plot the distribution of true values on the test set and the predicted values on the test set.\n",
    "\n",
    "figsize=(8, 8)\n",
    "\n",
    "# Density plot of the final predictions and the test values\n",
    "sns.kdeplot(final_pred, label = 'Predictions')\n",
    "sns.kdeplot(y_test, label = 'Values')\n",
    "\n",
    "# Label the plot\n",
    "plt.xlabel('Energy Star Score'); plt.ylabel('Density');\n",
    "plt.title('Test Values and Predictions');\n",
    "\n",
    "# The distribution looks to be nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8235d97848084acb96059768756b41310b1791d9"
   },
   "outputs": [],
   "source": [
    "# Another diagnostic plot is a histogram of the residuals. \n",
    "# Ideally, we would hope that the residuals are normally distributed, meaning that the model is wrong the same amount in both directions (high and low).\n",
    "\n",
    "figsize = (6, 6)\n",
    "\n",
    "# Calculate the residuals \n",
    "residuals = final_pred - y_test\n",
    "\n",
    "# Plot the residuals in a histogram\n",
    "plt.hist(residuals, color = 'red', bins = 20,\n",
    "         edgecolor = 'black')\n",
    "plt.xlabel('Error'); plt.ylabel('Count')\n",
    "plt.title('Distribution of Residuals');\n",
    "\n",
    "# The residuals are very close to zero value, with a one noticeable outliers on the low end. \n",
    "# These indicate errors where the model estimate was far below that of the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "309148f882872f058d9133c8c4e63826138b58ca"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01603b280ddc008608d8f00f5f9cf035fea4f07e"
   },
   "outputs": [],
   "source": [
    "# # # Feature Importances\n",
    "\n",
    "# One of the basic ways we can interpret an ensemble of decision trees is through what are known as the feature importances. \n",
    "# These can be interpreted as the variables which are most predictive of the target.\n",
    "\n",
    "# Extracting the feature importances from a trained ensemble of trees is quite easy in scikit-learn. \n",
    "# We will store the feature importances in a dataframe to analyze and visualize them.\n",
    "\n",
    "# Extract the feature importances into a dataframe\n",
    "feature_results = pd.DataFrame({'feature': list(features.columns), \n",
    "                                'importance': model.feature_importances_})\n",
    "\n",
    "# Show the top 10 most important\n",
    "feature_results = feature_results.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "feature_results.head(10)\n",
    "\n",
    "# The Credit Score, Current Loan Amount and Annual Income are the three most important features by quite a large margin. After that, Maximum Open Credit, Monthly Debt and Years of Credit History have\n",
    "# almost the same importance and, Home Ownership_Home Mortgage and Term_Long Term have almost the same importance as well. It indicates that we might not need to retain all of the features \n",
    "# to create a model with nearly the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3b17fbc413c19433b5bfe4bfcfb905e4d15e49d"
   },
   "outputs": [],
   "source": [
    "# Let's graph the feature importances to compare visually.\n",
    "\n",
    "figsize=(12, 10)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the 10 most important features in a horizontal bar chart\n",
    "feature_results.loc[:9, :].plot(x = 'feature', y = 'importance', \n",
    "                                 edgecolor = 'k',\n",
    "                                 kind='barh', color = 'blue');\n",
    "plt.xlabel('Relative Importance', size = 20); plt.ylabel('')\n",
    "plt.title('Feature Importances from Random Forest', size = 30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8d1b749cf7a33872d29ed4b474fa5ef3f4cc919"
   },
   "outputs": [],
   "source": [
    "# # Use Feature Importances for Feature Selection\n",
    "\n",
    "# Given that not every feature is important for finding the score, what would happen if we used a simpler model, such as a logistic regression, with the subset of most important features from the \n",
    "# random forest? The logistic regression did outperform the baseline, but it did not perform well compared to the model complex models. \n",
    "# Let's try using only the 6 most important features in the logistic regression to see if performance is improved. We can also limit to these features and re-evaluate the random forest.\n",
    "\n",
    "# Extract the names of the most important features\n",
    "most_important_features = feature_results['feature'][:10]\n",
    "\n",
    "# Find the index that corresponds to each feature name\n",
    "indices = [list(features.columns).index(x) for x in most_important_features]\n",
    "\n",
    "# Keep only the most important features\n",
    "X_train_reduced = X_train[:, indices]\n",
    "X_test_reduced = X_test[:, indices]\n",
    "\n",
    "print('Most important training features shape: ', X_train_reduced.shape)\n",
    "print('Most important testing  features shape: ', X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62651e122078dcbaa967e73249601371aab0ee5b"
   },
   "outputs": [],
   "source": [
    "logr = LogisticRegression()\n",
    "\n",
    "# Fit on full set of features\n",
    "logr.fit(X_train, y_train)\n",
    "logr_full_pred = logr.predict(X_test)\n",
    "\n",
    "# Fit on reduced set of features\n",
    "logr.fit(X_train_reduced, y_train)\n",
    "logr_reduced_pred = logr.predict(X_test_reduced)\n",
    "\n",
    "# Display results\n",
    "print('Logistic Regression Full Results: MAE =    %0.4f.' % mae(y_test, logr_full_pred))\n",
    "print('Logistic Regression Reduced Results: MAE = %0.4f.' % mae(y_test, logr_reduced_pred))\n",
    "\n",
    "# Well, reducing the features did improve the linear regression results little better! \n",
    "# It turns out that the extra information in the features with soft importance do actually improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0a03897e8d9cabb0b43f8dc18b7d832721802b4"
   },
   "outputs": [],
   "source": [
    "# Let's look at using the reduced set of features in the gradient boosted regressor. How is the performance affected?\n",
    "\n",
    "# Create the model with the same hyperparamters\n",
    "model_reduced = XGBClassifier(loss='ls', max_depth=5, max_features='auto',\n",
    "                                  min_samples_leaf=6, min_samples_split=2, \n",
    "                                  n_estimators=450, random_state=42)\n",
    "\n",
    "# Fit and test on the reduced set of features\n",
    "model_reduced.fit(X_train_reduced, y_train)\n",
    "model_reduced_pred = model_reduced.predict(X_test_reduced)\n",
    "\n",
    "print('Gradient Boosted Reduced Results: MAE = %0.4f' % mae(y_test, model_reduced_pred))\n",
    "\n",
    "# The model results are slightly worse with the reduced set of features and we will keep all of the features for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1e10affb766c994ab0f06509c061e9f78a0c249"
   },
   "outputs": [],
   "source": [
    "# # Locally Interpretable Model-agnostic Explanations (LIME)\n",
    "\n",
    "# We will look at using LIME to explain individual predictions made the by the model. \n",
    "#LIME is a relatively new effort aimed at showing how a machine learning model thinks by approximating the region around a prediction with a linear model.\n",
    "\n",
    "# We will look at trying to explain the predictions on an example the model gets very wrong and an example the model gets correct. \n",
    "#We will restrict ourselves to using the reduced set of 10 features to aid interpretability. \n",
    "#The model trained on the 10 most important features is slightly less accurate, but we generally have to trade off accuracy for interpretability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe7fce785eb534a560c6ca6e7446babf5e81c042"
   },
   "outputs": [],
   "source": [
    "# Find the residuals\n",
    "residuals = abs(model_reduced_pred - y_test)\n",
    "    \n",
    "# Exact the worst and best prediction\n",
    "wrong = X_test_reduced[np.argmax(residuals), :]\n",
    "right = X_test_reduced[np.argmin(residuals), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01e95559f6c5b0a269de6f220f297e58f64f4245"
   },
   "outputs": [],
   "source": [
    "# Create a lime explainer object\n",
    "\n",
    "# LIME for explaining predictions\n",
    "import lime \n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data = X_train_reduced, \n",
    "                                                   mode = 'classification',\n",
    "                                                   training_labels = y_train,\n",
    "                                                   feature_names = list(most_important_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e45da9ad22dcfc8e8a8484efe3d502b5f2f03df3"
   },
   "outputs": [],
   "source": [
    "# Display the predicted and true value for the wrong instance\n",
    "print('Prediction: %0.4f' % model_reduced.predict(wrong.reshape(1, -1)))\n",
    "print('Actual Value: %0.4f' % y_test[np.argmax(residuals)])\n",
    "\n",
    "# Explanation for wrong prediction\n",
    "wrong_exp = explainer.explain_instance(data_row = wrong, \n",
    "                                       predict_fn = model_reduced.predict_proba,\n",
    "                                      num_features=10)\n",
    "\n",
    "# Plot the prediction explaination\n",
    "wrong_exp.as_pyplot_figure();\n",
    "plt.title('Explanation of Prediction', size = 28);\n",
    "plt.xlabel('Effect on Prediction', size = 22);\n",
    "\n",
    "# In this example, our gradient boosted model predicted a score of 1.0 and the actual value was 0.0.\n",
    "\n",
    "# The plot from LIME is showing us the contribution to the final prediction from each of the features for the example.\n",
    "\n",
    "# We can see that the Credit Score singificantly decreased the prediction when we comparing with the others. \n",
    "# The Maximum Open Credit on the other hand, incresed the prediction when we comparing with the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b281086bbc210fa9d65fda37268ca79c9007bc4d"
   },
   "outputs": [],
   "source": [
    "# Now we can go through the same process with a prediction the model got correct.\n",
    "\n",
    "# Display the predicted and true value for the wrong instance\n",
    "print('Prediction: %0.4f' % model_reduced.predict(right.reshape(1, -1)))\n",
    "print('Actual Value: %0.4f' % y_test[np.argmin(residuals)])\n",
    "\n",
    "# Explanation for wrong prediction\n",
    "right_exp = explainer.explain_instance(right, model_reduced.predict_proba, num_features=10)\n",
    "right_exp.as_pyplot_figure();\n",
    "plt.title('Explanation of Prediction', size = 28);\n",
    "plt.xlabel('Effect on Prediction', size = 22);\n",
    "\n",
    "# The correct value for this case was 1.0 which our gradient boosted model got right on!\n",
    "\n",
    "# The plot from LIME again shows the contribution to the prediciton of each of feature variables for the example.\n",
    "\n",
    "# Observing break down plots like these allow us to get an idea of how the model makes a prediction. \n",
    "# This is probably most valuable for cases where the model is off by a large amount as we can inspect the errors and perhaps engineer better features or adjust the hyperparameters of the model \n",
    "# to improve predictions for next time. The examples where the model is off the most could also be interesting edge cases to look at manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d99aafb8327e9ea54a06afd92f482991a24d8fb6"
   },
   "outputs": [],
   "source": [
    "# A process such as this where we try to work with the machine learning algorithm to gain understanding of a problem seems much better than simply letting the model make predictions\n",
    "# and completely trusting them! Although LIME is not perfect, it represents a step in the right direction towards explaining machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
