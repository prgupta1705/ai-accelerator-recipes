{"cells":[{"metadata":{"_uuid":"f6285d85a00d6a31792fac4c08ba523cd049cf07","trusted":true},"cell_type":"code","source":"# I will use in this Kernel the step-by-step process of Will Koehrsen.\n# I won't use everything, but most of them.\n# This project at in GitHub repository: https://github.com/WillKoehrsen/machine-learning-project-walkthrough","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"##### # Imports\n\n# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# No warnings about setting value on copy of slice\npd.options.mode.chained_assignment = None\n\n# Display up to 60 columns of a dataframe\npd.set_option('display.max_columns', 60)\n\n# Matplotlib visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Set default font size\nplt.rcParams['font.size'] = 24\n\n# Internal ipython tool for setting figure size\nfrom IPython.core.pylabtools import figsize\n\n# Seaborn for visualization\nimport seaborn as sns\nsns.set(font_scale = 2)\n\n# Splitting data into training and testing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# # # Data Cleaning and Formatting\n\n# # Load in the Data and Examine\n\n# Read in credit into a dataframe \ncredit = pd.read_csv('../input/credit_train.csv')\n\n# Display top of dataframe\ncredit.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dd5c8d62615e1e852c59db262dde7fef027ca06","trusted":true},"cell_type":"code","source":"credit.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2a8dd84e67164754d6d43902592a8249ec6dac8","trusted":true},"cell_type":"code","source":"# # Data Types and Missing Values\n\n# See the column data types and non-missing values\ncredit.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c722bd0380292d587e258da5e3ff09d00f57bca","trusted":true},"cell_type":"code","source":"# Statistics for each column\ncredit.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e516824b86742c41bce64e70d6134231e31a4ea3","trusted":true},"cell_type":"code","source":"credit.drop(labels=['Loan ID', 'Customer ID'], axis=1, inplace=True)\n\n# These two features are only for identification.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d448aee129b1a9d2dfbfe60e7ffa71c3011689","trusted":true},"cell_type":"code","source":"# # Missing Values\n\n# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"254bd2f8d7e4e8082f412dfe7660f63dad43dc14","trusted":true},"cell_type":"code","source":"missing_values_table(credit)\n\n# A curious thing about the table below is the last 10 features have the same number o missing values.\n# I will go deeper and figure out what is happening.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ccf316384829ebc2b01d27ccf4b40aef37403c","trusted":true},"cell_type":"code","source":"# Drop the columns with > 50% missing\ncredit.drop(columns = 'Months since last delinquent', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"330d0ef3e7e0732b90dccaeb56d8ce949b48b509","scrolled":true,"trusted":true},"cell_type":"code","source":"credit[credit['Years of Credit History'].isnull() == True]\n\n# Here I can see that the last 514 observations are NaN values.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72df239d275ad726663669430043430d43477bec","trusted":true},"cell_type":"code","source":"credit.drop(credit.tail(514).index, inplace=True) # drop last 514 rows\nmissing_values_table(credit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f37c96f849e1a0505bf6dab39e377d115957662a","trusted":true},"cell_type":"code","source":"# As the number of missing values is so low in the 'Maximum Open Credit' I will drop them.\n\nfor i in credit['Maximum Open Credit'][credit['Maximum Open Credit'].isnull() == True].index:\n    credit.drop(labels=i, inplace=True)\nmissing_values_table(credit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1400de3e7f5be2188f94b6b41723ce8976adc3d2","trusted":true},"cell_type":"code","source":"# As the number of missing values is so low in the 'Tax Liens' I will drop them.\n\nfor i in credit['Tax Liens'][credit['Tax Liens'].isnull() == True].index:\n    credit.drop(labels=i, inplace=True)\nmissing_values_table(credit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61a666542aade522dbb48c908dcf7caf7ecd603d","trusted":true},"cell_type":"code","source":"# As the number of missing values is so low in the 'Bankruptcies' I will drop them.\n\nfor i in credit['Bankruptcies'][credit['Bankruptcies'].isnull() == True].index:\n    credit.drop(labels=i, inplace=True)\nmissing_values_table(credit)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a33a945216de8d42bdbf3731fdfe2d439693ee9","trusted":true},"cell_type":"code","source":"# Now I will use the 'mean' technique to fill the NaN values.\n\ncredit.fillna(credit.mean(), inplace=True)\nmissing_values_table(credit)\n\n# The feature 'Years in current job' didn't fill because has categorical values.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"430680323b51481400716a6598c95ec555ec8b55","trusted":true},"cell_type":"code","source":"# I will figure out what value is more present in this feature.\n\nplt.figure(figsize=(20,8))\n\nsns.countplot(credit['Years in current job'])\n\n# We can see that the value '10+ years' is strongly present in this feature, so I will use this value to fill the \n# missing values.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89356ed83e426d2412294230cbcf0d3d5297aa01","trusted":true},"cell_type":"code","source":"credit.fillna('10+ years', inplace=True) # fill with '10+ years'.\nmissing_values_table(credit)\n\n# No missing values anymore.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfbfe9c6e65a4de3890e5a3c4d56142c9dc73703","scrolled":false,"trusted":true},"cell_type":"code","source":"# # # Exploratory Data Analysis\n\nsns.pairplot(credit)\n\n# We can see in this pairplot graph that this dataset is extremely concentraded is 'zero' value, so here I won't\n# use the overfitting method.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fac1793f054519d8f11c418cd1b383b73d4777a","trusted":true},"cell_type":"code","source":"# # Correlations between Features and Target\n\n# Find all correlations and sort \ncorrelations_data = credit.corr()['Credit Score'].sort_values(ascending=False)\n\n# Print the correlations\nprint(correlations_data.tail)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ef51251b405bfecdc0e6c481f6a54eecd009c94","trusted":true},"cell_type":"code","source":"# # # Feature Engineering and Selection\n\ncredit.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13e110a0d27ac1da28d562ad0b6d84d8bfd4cd75","trusted":true},"cell_type":"code","source":"# # Encoding categorical data & Feature Scaling\n\n# Select the categorical columns\ncategorical_subset = credit[['Term', 'Years in current job', 'Home Ownership', 'Purpose']]\n\n# One hot encode\ncategorical_subset = pd.get_dummies(categorical_subset)\n\n# Join the dataframe in credit_train\n# Make sure to use axis = 1 to perform a column bind\n# First I will drop the 'old' categorical datas and after I will join the 'new' one.\n\ncredit.drop(labels=['Term', 'Years in current job', 'Home Ownership', 'Purpose'], axis=1, inplace=True)\ncredit = pd.concat([credit, categorical_subset], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07ba1398ad974d62ccd1c65a1981915abc6976f","trusted":true},"cell_type":"code","source":"# #  Remove Collinear Features\n\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model\n        to generalize and improves the interpretability of the model.\n        \n    Inputs: \n        threshold: any features with correlations greater than this value are removed\n    \n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n    \n    # Dont want to remove correlations between Energy Star Score\n    y = x['Loan Status']\n    x = x.drop(columns = ['Loan Status'])\n    \n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n            \n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns = drops)\n    \n    # Add the score back in to the data\n    x['Loan Status'] = y\n               \n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ed921620417e13fb5641564b5090b3d7e97d86d","trusted":true},"cell_type":"code","source":"# Remove the collinear features above a specified correlation coefficient\ncredit = remove_collinear_features(credit, 0.6);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4be0e65c2ede81decd769b8e8768194a3c56b4cf","trusted":true},"cell_type":"code","source":"credit.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"241e10723db0c1fd8df28e7afbac69d8b2b4ab1b","trusted":true},"cell_type":"code","source":"# # # Split Into Training and Testing Sets\n\n# Separate out the features and targets\nfeatures = credit.drop(columns='Loan Status')\ntargets = pd.DataFrame(credit['Loan Status'])\n\n# Split into 80% training and 20% testing set\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size = 0.2, random_state = 42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"349e486f1e8525e364f6be5334291ae8f00ca0cd","trusted":true},"cell_type":"code","source":"# # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Encoding the Dependent Variable\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_y_train = LabelEncoder()\ny_train = labelencoder_y_train.fit_transform(y_train)\nlabelencoder_y_test = LabelEncoder()\ny_test = labelencoder_y_test.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5fb7a46a004a2ec0570c4c135088a447c825d67","trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3eadeec18118a0f19acb201dc89d2d617617c8e","trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fc8657bd8f88e32c876056e8279a1982a7d51bd","trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"102f279c83b538955a51028ea6b589cd9c75d293","trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03122062d1f1f41806b73d724ecc2cf403894b06","trusted":true},"cell_type":"code","source":"# # # Models to Evaluate\n\n# We will compare five different machine learning Classification models:\n\n# 1 - Logistic Regression\n# 2 - K-Nearest Neighbors Classification\n# 3 - Suport Vector Machine\n# 4 - Naive Bayes\n# 5 - Random Forest Classification\n\n# Function to calculate mean absolute error\ndef cross_val(X_train, y_train, model):\n    # Applying k-Fold Cross Validation\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n    return accuracies.mean()\n\n# Takes in a model, trains the model, and evaluates the model on the test set\ndef fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(X_test)\n    model_cross = cross_val(X_train, y_train, model)\n    \n    # Return the performance metric\n    return model_cross","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8fa7e51ce560cc110a94fd0d3988f7eefa6f26d","scrolled":false,"trusted":true},"cell_type":"code","source":"# # Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nlogr_cross = fit_and_evaluate(logr)\n\nprint('Logistic Regression Performance on the test set: Cross Validation Score = %0.4f' % logr_cross)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60e1016bc91b18ebabd1b0cac4585260d5ccd924","trusted":true},"cell_type":"code","source":"# # K-NN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_cross = fit_and_evaluate(knn)\n\nprint('KNN Performance on the test set: Cross Validation Score = %0.4f' % knn_cross)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"760b7b5bb05d9f130246180ca97c21ddcd67600c","trusted":true},"cell_type":"code","source":"# # Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnaive = GaussianNB()\nnaive_cross = fit_and_evaluate(naive)\n\nprint('Naive Bayes Performance on the test set: Cross Validation Score = %0.4f' % naive_cross)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae4b09b6570c6aa3f9928bc21ae93dbe7066362c","trusted":true},"cell_type":"code","source":"# # Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrandom = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nrandom_cross = fit_and_evaluate(random)\n\nprint('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d98eaec33f4e6cadd299ec586ab840020952b6fb","trusted":true},"cell_type":"code","source":"# # Gradiente Boosting Classification\nfrom xgboost import XGBClassifier\ngb = XGBClassifier()\ngb_cross = fit_and_evaluate(gb)\n\nprint('Gradiente Boosting Classification Performance on the test set: Cross Validation Score = %0.4f' % gb_cross)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4413692210395764ecf6bdaa68678983b7106bc","trusted":true},"cell_type":"code","source":"# Now, to better understand the results, I will show in a graph the model that has the better Cross Validation Score\n\nplt.style.use('fivethirtyeight')\nfigsize=(8, 6)\n\n# Dataframe to hold the results\nmodel_comparison = pd.DataFrame({'model': ['Logistic Regression', 'K-NN',\n                                           'Naive Bayes', 'Random Forest',\n                                            'Gradiente Boosting'],\n                                 'score': [logr_cross, knn_cross, naive_cross, \n                                         random_cross, gb_cross]})\n\n# Horizontal bar chart of test mae\nmodel_comparison.sort_values('score', ascending = True).plot(x = 'model', y = 'score', kind = 'barh',\n                                                           color = 'red', edgecolor = 'black')\n\n# Plot formatting\nplt.ylabel(''); plt.yticks(size = 14); plt.xlabel('K-Fold Cross Validation'); plt.xticks(size = 14)\nplt.title('Model Comparison on Score', size = 20);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68c0e5f787870535c909005d66113a2651d12830","trusted":true},"cell_type":"code","source":"# # # Model Optimization\n\n# # Hyperparameter\n\n# Hyperparameter Tuning with Random Search and Cross Validation\n\n# Here we will implement random search with cross validation to select the optimal hyperparameters for the gradient boosting regressor. \n# We first define a grid then peform an iterative process of: randomly sample a set of hyperparameters from the grid, evaluate the hyperparameters using 4-fold cross-validation, \n# and then select the hyperparameters with the best performance.\n\n# Loss function to be optimized\nloss = ['ls', 'lad', 'huber']\n\n# Number of trees used in the boosting process\nn_estimators = [100, 500, 900, 1100, 1500]\n\n# Maximum depth of each tree\nmax_depth = [2, 3, 5, 10, 15]\n\n# Minimum number of samples per leaf\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'loss': loss,\n                       'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea9ec545e6debd1c3b5c726ab5a63b5230b1955e","trusted":true},"cell_type":"code","source":"# In the code below, we create the Randomized Search Object passing in the following parameters:\n\n#    estimator: the model\n#    param_distributions: the distribution of parameters we defined\n#    cv the number of folds to use for k-fold cross validation\n#    n_iter: the number of different combinations to try\n#    scoring: which metric to use when evaluating candidates\n#    n_jobs: number of cores to run in parallel (-1 will use all available)\n#    verbose: how much information to display (1 displays a limited amount)\n#    return_train_score: return the training score for each cross-validation fold\n#    random_state: fixes the random number generator used so we get the same results every run","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8d4b598cb6115f8afa29fed29a1353bcb4ad94","scrolled":true,"trusted":true},"cell_type":"code","source":"# The Randomized Search Object is trained the same way as any other scikit-learn model. \n# After training, we can compare all the different hyperparameter combinations and find the best performing one.\n\n# Create the model to use for hyperparameter tuning\nmodel = XGBClassifier(random_state = 42)\n\n# Set up the random search with 4-fold cross validation\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=4, n_iter=25, \n                               scoring = 'neg_mean_absolute_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72cd906f1b667e12946a44102979d8ef55981c7f","trusted":true},"cell_type":"code","source":"# Fit on the training data\nrandom_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd8d837b0f6bd084cf29090a89f1fc4368f1df64","trusted":true},"cell_type":"code","source":"# Scikit-learn uses the negative mean absolute error for evaluation because it wants a metric to maximize. \n# Therefore, a better score will be closer to 0. We can get the results of the randomized search into a dataframe, and sort the values by performance.\n\n# Get all of the cv results and sort by the test performance\nrandom_results = pd.DataFrame(random_cv.cv_results_).sort_values('mean_test_score', ascending = False)\n\nrandom_results.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"922388d85784fec033d756c9c0843c0c91a759e7","trusted":true},"cell_type":"code","source":"random_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d83a5cec0d161170d31dfa2b46a0d674a787c4d0","trusted":true},"cell_type":"code","source":"# The best gradient boosted model has the following hyperparameters:\n\n# loss = ls\n# n_estimators = 100\n# max_depth = 5\n# min_samples_leaf = 6\n# min_samples_split = 2\n# max_features = auto ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3db261e84daf6c4b9777588879ca0a92c8a5e632","trusted":true},"cell_type":"code","source":"# I will focus on a single one, the number of trees in the forest (n_estimators).\n# By varying only one hyperparameter, we can directly observe how it affects performance. \n# In the case of the number of trees, we would expect to see a significant affect on the amount of under vs overfitting.\n\n# Here we will use grid search with a grid that only has the n_estimators hyperparameter. \n# We will evaluate a range of trees then plot the training and testing performance to get an idea of what increasing the number of trees does for our model. \n# We will fix the other hyperparameters at the best values returned from random search to isolate the number of trees effect.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c59406726362d253d910af2f3103608a4a1ab847","trusted":true},"cell_type":"code","source":"# Create a range of trees to evaluate\ntrees_grid = {'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]}\n\nmodel = XGBClassifier(loss = 'ls', max_depth = 5,\n                                  min_samples_leaf = 6,\n                                  min_samples_split = 2,\n                                  max_features = 'auto',\n                                  random_state = 42)\n\n# Grid Search Object using the trees range and the random forest model\ngrid_search = GridSearchCV(estimator = model, param_grid=trees_grid, cv = 4, \n                           scoring = 'neg_mean_absolute_error', verbose = 1,\n                           n_jobs = -1, return_train_score = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ea5130e0e4e841bd3265f109eaaf0b457edc4ba","trusted":true},"cell_type":"code","source":"# Fit the grid search\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab0245973e91237f0eec4f844438b4acc9d62e56","trusted":true},"cell_type":"code","source":"# Get the results into a dataframe\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Plot the training and testing error vs number of trees\nfigsize=(8, 8)\nplt.style.use('fivethirtyeight')\nplt.plot(results['param_n_estimators'], -1 * results['mean_test_score'], label = 'Testing Error')\nplt.plot(results['param_n_estimators'], -1 * results['mean_train_score'], label = 'Training Error')\nplt.xlabel('Number of Trees'); plt.ylabel('Mean Abosolute Error'); plt.legend();\nplt.title('Performance vs Number of Trees');\n\n# There will always be a difference between the training error and testing error (the training error is always lower) but if there is a significant difference, \n# we want to try and reduce overfitting, either by getting more training data or reducing the complexity of the model through hyperparameter tuning or regularization.\n\n# For now, we will use the model with the best performance and accept that it may be overfitting to the training set.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e28c06eb71850cc45c974d92ae0cd28ce040b2","trusted":true},"cell_type":"code","source":"results.sort_values('mean_test_score', ascending = False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70d076ed10f7ae5b7f280d4e06c6b6d46999d5b3","trusted":true},"cell_type":"code","source":"# # # Evaluate Final Model on the Test Set\n\n# We will use the best model from hyperparameter tuning to make predictions on the testing set.\n\n# For comparison, we can also look at the performance of the default model. The code below creates the final model, trains it (with timing), and evaluates on the test set.\n\n# Default model\ndefault_model = XGBClassifier(random_state = 42)\n\n# Select the best model\nfinal_model = grid_search.best_estimator_\n\nfinal_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f57808f6cc35053386fe04a29181bb31bd5d7468","trusted":true},"cell_type":"code","source":"%%timeit -n 1 -r 5\ndefault_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0679eeb425ab9195402cbc9ec0b1604981aa65de","trusted":true},"cell_type":"code","source":"%%timeit -n 1 -r 5\nfinal_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e22420e5d2d8928d6364322ab1f06c5db5fd9d77","trusted":true},"cell_type":"code","source":"default_pred = default_model.predict(X_test)\nfinal_pred = final_model.predict(X_test)\n\n# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))\n\nprint('Default model performance on the test set: MAE = %0.4f.' % mae(y_test, default_pred))\nprint('Final model performance on the test set:   MAE = %0.4f.' % mae(y_test, final_pred))\n\n# The final model does out-perform the baseline model by about less than 1% and at the cost of significantly increased running time (it's about 7 times slower on my machine). \n\n# Here, the increase in run time is not an impediment, because while the relative difference is very small, the absolute magnitude of the training time is not significant. \n# In a different situation, the balance might not be the same so we would need to consider what we are optimizing for and the limitations we have to work with.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b59237f724d86418c50cf353a94fcd44f4abd08","trusted":true},"cell_type":"code","source":"# To get a sense of the predictions, we can plot the distribution of true values on the test set and the predicted values on the test set.\n\nfigsize=(8, 8)\n\n# Density plot of the final predictions and the test values\nsns.kdeplot(final_pred, label = 'Predictions')\nsns.kdeplot(y_test, label = 'Values')\n\n# Label the plot\nplt.xlabel('Energy Star Score'); plt.ylabel('Density');\nplt.title('Test Values and Predictions');\n\n# The distribution looks to be nearly the same.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8235d97848084acb96059768756b41310b1791d9","trusted":true},"cell_type":"code","source":"# Another diagnostic plot is a histogram of the residuals. \n# Ideally, we would hope that the residuals are normally distributed, meaning that the model is wrong the same amount in both directions (high and low).\n\nfigsize = (6, 6)\n\n# Calculate the residuals \nresiduals = final_pred - y_test\n\n# Plot the residuals in a histogram\nplt.hist(residuals, color = 'red', bins = 20,\n         edgecolor = 'black')\nplt.xlabel('Error'); plt.ylabel('Count')\nplt.title('Distribution of Residuals');\n\n# The residuals are very close to zero value, with a one noticeable outliers on the low end. \n# These indicate errors where the model estimate was far below that of the true value.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"309148f882872f058d9133c8c4e63826138b58ca","trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01603b280ddc008608d8f00f5f9cf035fea4f07e","trusted":true},"cell_type":"code","source":"# # # Feature Importances\n\n# One of the basic ways we can interpret an ensemble of decision trees is through what are known as the feature importances. \n# These can be interpreted as the variables which are most predictive of the target.\n\n# Extracting the feature importances from a trained ensemble of trees is quite easy in scikit-learn. \n# We will store the feature importances in a dataframe to analyze and visualize them.\n\n# Extract the feature importances into a dataframe\nfeature_results = pd.DataFrame({'feature': list(features.columns), \n                                'importance': model.feature_importances_})\n\n# Show the top 10 most important\nfeature_results = feature_results.sort_values('importance', ascending = False).reset_index(drop=True)\n\nfeature_results.head(10)\n\n# The Credit Score, Current Loan Amount and Annual Income are the three most important features by quite a large margin. After that, Maximum Open Credit, Monthly Debt and Years of Credit History have\n# almost the same importance and, Home Ownership_Home Mortgage and Term_Long Term have almost the same importance as well. It indicates that we might not need to retain all of the features \n# to create a model with nearly the same performance.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3b17fbc413c19433b5bfe4bfcfb905e4d15e49d","trusted":true},"cell_type":"code","source":"# Let's graph the feature importances to compare visually.\n\nfigsize=(12, 10)\nplt.style.use('fivethirtyeight')\n\n# Plot the 10 most important features in a horizontal bar chart\nfeature_results.loc[:9, :].plot(x = 'feature', y = 'importance', \n                                 edgecolor = 'k',\n                                 kind='barh', color = 'blue');\nplt.xlabel('Relative Importance', size = 20); plt.ylabel('')\nplt.title('Feature Importances from Random Forest', size = 30);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8d1b749cf7a33872d29ed4b474fa5ef3f4cc919","trusted":true},"cell_type":"code","source":"# # Use Feature Importances for Feature Selection\n\n# Given that not every feature is important for finding the score, what would happen if we used a simpler model, such as a logistic regression, with the subset of most important features from the \n# random forest? The logistic regression did outperform the baseline, but it did not perform well compared to the model complex models. \n# Let's try using only the 6 most important features in the logistic regression to see if performance is improved. We can also limit to these features and re-evaluate the random forest.\n\n# Extract the names of the most important features\nmost_important_features = feature_results['feature'][:10]\n\n# Find the index that corresponds to each feature name\nindices = [list(features.columns).index(x) for x in most_important_features]\n\n# Keep only the most important features\nX_train_reduced = X_train[:, indices]\nX_test_reduced = X_test[:, indices]\n\nprint('Most important training features shape: ', X_train_reduced.shape)\nprint('Most important testing  features shape: ', X_test_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62651e122078dcbaa967e73249601371aab0ee5b","trusted":true},"cell_type":"code","source":"logr = LogisticRegression()\n\n# Fit on full set of features\nlogr.fit(X_train, y_train)\nlogr_full_pred = logr.predict(X_test)\n\n# Fit on reduced set of features\nlogr.fit(X_train_reduced, y_train)\nlogr_reduced_pred = logr.predict(X_test_reduced)\n\n# Display results\nprint('Logistic Regression Full Results: MAE =    %0.4f.' % mae(y_test, logr_full_pred))\nprint('Logistic Regression Reduced Results: MAE = %0.4f.' % mae(y_test, logr_reduced_pred))\n\n# Well, reducing the features did improve the linear regression results little beat! \n# It turns out that the extra information in the features with soft importance do actually improve performance.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0a03897e8d9cabb0b43f8dc18b7d832721802b4","trusted":true},"cell_type":"code","source":"# Let's look at using the reduced set of features in the gradient boosted regressor. How is the performance affected?\n\n# Create the model with the same hyperparamters\nmodel_reduced = XGBClassifier(loss='ls', max_depth=5, max_features='auto',\n                                  min_samples_leaf=6, min_samples_split=2, \n                                  n_estimators=450, random_state=42)\n\n# Fit and test on the reduced set of features\nmodel_reduced.fit(X_train_reduced, y_train)\nmodel_reduced_pred = model_reduced.predict(X_test_reduced)\n\nprint('Gradient Boosted Reduced Results: MAE = %0.4f' % mae(y_test, model_reduced_pred))\n\n# The model results are slightly worse with the reduced set of features and we will keep all of the features for the final model.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e10affb766c994ab0f06509c061e9f78a0c249","trusted":true},"cell_type":"code","source":"# # Locally Interpretable Model-agnostic Explanations (LIME)\n\n# We will look at using LIME to explain individual predictions made the by the model. \n#LIME is a relatively new effort aimed at showing how a machine learning model thinks by approximating the region around a prediction with a linear model.\n\n# We will look at trying to explain the predictions on an example the model gets very wrong and an example the model gets correct. \n#We will restrict ourselves to using the reduced set of 10 features to aid interpretability. \n#The model trained on the 10 most important features is slightly less accurate, but we generally have to trade off accuracy for interpretability!","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7fce785eb534a560c6ca6e7446babf5e81c042","trusted":true},"cell_type":"code","source":"# Find the residuals\nresiduals = abs(model_reduced_pred - y_test)\n    \n# Exact the worst and best prediction\nwrong = X_test_reduced[np.argmax(residuals), :]\nright = X_test_reduced[np.argmin(residuals), :]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01e95559f6c5b0a269de6f220f297e58f64f4245","trusted":true},"cell_type":"code","source":"# Create a lime explainer object\n\n# LIME for explaining predictions\nimport lime \nimport lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(training_data = X_train_reduced, \n                                                   mode = 'classification',\n                                                   training_labels = y_train,\n                                                   feature_names = list(most_important_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e45da9ad22dcfc8e8a8484efe3d502b5f2f03df3","trusted":true},"cell_type":"code","source":"# Display the predicted and true value for the wrong instance\nprint('Prediction: %0.4f' % model_reduced.predict(wrong.reshape(1, -1)))\nprint('Actual Value: %0.4f' % y_test[np.argmax(residuals)])\n\n# Explanation for wrong prediction\nwrong_exp = explainer.explain_instance(data_row = wrong, \n                                       predict_fn = model_reduced.predict_proba,\n                                      num_features=10)\n\n# Plot the prediction explaination\nwrong_exp.as_pyplot_figure();\nplt.title('Explanation of Prediction', size = 28);\nplt.xlabel('Effect on Prediction', size = 22);\n\n# In this example, our gradient boosted model predicted a score of 1.0 and the actual value was 0.0.\n\n# The plot from LIME is showing us the contribution to the final prediction from each of the features for the example.\n\n# We can see that the Credit Score singificantly decreased the prediction when we comparing with the others. \n# The Maximum Open Credit on the other hand, incresed the prediction when we comparing with the others.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b281086bbc210fa9d65fda37268ca79c9007bc4d","trusted":true},"cell_type":"code","source":"# Now we can go through the same process with a prediction the model got correct.\n\n# Display the predicted and true value for the wrong instance\nprint('Prediction: %0.4f' % model_reduced.predict(right.reshape(1, -1)))\nprint('Actual Value: %0.4f' % y_test[np.argmin(residuals)])\n\n# Explanation for wrong prediction\nright_exp = explainer.explain_instance(right, model_reduced.predict_proba, num_features=10)\nright_exp.as_pyplot_figure();\nplt.title('Explanation of Prediction', size = 28);\nplt.xlabel('Effect on Prediction', size = 22);\n\n# The correct value for this case was 1.0 which our gradient boosted model got right on!\n\n# The plot from LIME again shows the contribution to the prediciton of each of feature variables for the example.\n\n# Observing break down plots like these allow us to get an idea of how the model makes a prediction. \n# This is probably most valuable for cases where the model is off by a large amount as we can inspect the errors and perhaps engineer better features or adjust the hyperparameters of the model \n# to improve predictions for next time. The examples where the model is off the most could also be interesting edge cases to look at manually.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d99aafb8327e9ea54a06afd92f482991a24d8fb6","trusted":true},"cell_type":"code","source":"# A process such as this where we try to work with the machine learning algorithm to gain understanding of a problem seems much better than simply letting the model make predictions\n# and completely trusting them! Although LIME is not perfect, it represents a step in the right direction towards explaining machine learning models.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73c2b9dc497155d2e81ca8c82da9ba1e77e0f71d","trusted":true},"cell_type":"code","source":"# Good job with this project!\n# See you in the next one!!!","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34a9898eca390491966577e7ac84dffcc4f1fb68","trusted":true},"cell_type":"code","source":"# I will use in this Kernel the step-by-step process of Will Koehrsen.\n# I won't use everything, but most of them.\n# This project at in GitHub repository: https://github.com/WillKoehrsen/machine-learning-project-walkthrough","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}